{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299fe9d5",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "963049c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9fa8c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnbc = pd.read_csv(r'\\Users\\39118\\Desktop\\9665\\NLPdata\\cnbc_headlines.csv')\n",
    "# guar = pd.read_csv(r'\\Users\\39118\\Desktop\\9665\\NLPdata\\guardian_headlines.csv')\n",
    "# reut = pd.read_csv(r'\\Users\\39118\\Desktop\\9665\\NLPdata\\reuters_headlines.csv')\n",
    "# data = pd.concat([cnbc, guar, reut], axis = 0)\n",
    "# data.to_csv('data_combined.csv')\n",
    "# dataT = data.copy()\n",
    "# dataL = dataT[['Headlines','Time']].dropna()\n",
    "# dataL.shape\n",
    "# dataL['Year'] = [re.split('[-\\s]', var)[-1] for var in dataL['Time']]\n",
    "# dataL['Year'] = ['20'+ item if len(item) == 2 else item for item in dataL['Year']]\n",
    "# dataL['Year'] = pd.to_datetime(dataL['Year'], format = '%Y')\n",
    "# dataL = dataL.set_index('Year')\n",
    "# dataL.to_excel('News Dataset for Project.xlsx')\n",
    "data = pd.read_excel('News Dataset for Project.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "176695f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "16b1bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_data = news_data[['Headlines']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526ba10",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ee4509d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "27dff98a-d32d-49e7-9ddc-6a6807c839b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ad450244",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8b26f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build customized stopwords\n",
    "stopwords = ['i',\"'s'\",'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \n",
    "             \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "             'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "             'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "             'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', \n",
    "             'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "             'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', \n",
    "             'a', 'an', 'the', 'and', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "             'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "             'during', 'before', 'after', 'to', 'from', 'in', 'out', 'on', 'over', 'under', \n",
    "             'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n",
    "             'how', 'all', 'any', 'both', 'each', 'more', 'most', 'other', 'some', 'such',\n",
    "             'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren']\n",
    "negations = words[-35:].extend(['don',\"don't\",'few','up','down','under','above','against','off','not','no','nor'])\n",
    "punctuation = ['.','?',\";\",'...','\"','()','\\'',',','_','!','--']\n",
    "special_punc = [':']\n",
    "high_freq_useless = ['jim','cramer','said','us','ceo','say','says']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "970d296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-156-b1157b4f5183>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[name] = data[name].str.lower()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_news_data(data,name):\n",
    "    data[name] = data[name].str.lower()\n",
    "#     data[name] = data[name].apply(lambda x:re.sub(r'','not',x))\n",
    "preprocess_news_data(headlines_data,'Headlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "75c7f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer initialization\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d6797470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-158-29544a1c44fa>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headlines_data['Tokenized_Headlines'] = [nltk.word_tokenize(sent)\n"
     ]
    }
   ],
   "source": [
    "# tokenize new headlines\n",
    "headlines_data['Tokenized_Headlines'] = [nltk.word_tokenize(sent) \n",
    "                                         for sent in headlines_data.Headlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e53af525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-159-0177349d7759>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headlines_data['Tokenized_Headlines'] = Tokenized_Headlines\n"
     ]
    }
   ],
   "source": [
    "Tokenized_Headlines = []\n",
    "for i in headlines_data['Tokenized_Headlines']:\n",
    "    Tokenized_Headlines.append([word.strip(\"'\") for word in i])\n",
    "headlines_data['Tokenized_Headlines'] = Tokenized_Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "57d92c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace n't with not\n",
    "for i in headlines_data.Tokenized_Headlines:\n",
    "    for j in range(len(i)):\n",
    "        if i[j] == \"n't\" :\n",
    "            i[j] = 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "29ef65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_tokenized(tokens) -> list:\n",
    "    '''normalize,stem and remove stopwords'''\n",
    "    Normal_Tokenized_Headlines = []\n",
    "    for token in tokens:\n",
    "        Normal_Tokenized_Headlines.append([wnl.lemmatize(word).lower()\n",
    "                                           for word in token \n",
    "                                           if word.lower() not in stopwords and\n",
    "                                           word.isalpha() or word in special_punc]) \n",
    "    return Normal_Tokenized_Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "01c8ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_data['Normal_Tokenized_Headlines'] = normal_tokenized(headlines_data['Tokenized_Headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfedc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d9634801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(tokens) -> list:\n",
    "    '''remove irrelevant content and text before semicolon'''\n",
    "    index_colon_apply = []\n",
    "    index_say_apply = []\n",
    "    for i in tokens:\n",
    "        if ':' in i:\n",
    "            index_colon_apply.append(i.index(':'))\n",
    "        else:\n",
    "            index_colon_apply.append(0)\n",
    "        if 'say' in i:\n",
    "            index_say_apply.append(i.index('say'))\n",
    "        else:\n",
    "            index_say_apply.append(0)\n",
    "           \n",
    "    return index_colon_apply, index_say_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "57625513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the index for the point where may contain the useless information\n",
    "index_colon_apply, index_say_apply = index(headlines_data['Normal_Tokenized_Headlines'])\n",
    "headlines_data['index_colon_apply'] = index_colon_apply\n",
    "headlines_data['index_say_apply'] = index_say_apply\n",
    "headlines_data['tokens_len'] = [len(token)-1 for token in headlines_data['Normal_Tokenized_Headlines']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e8d4e79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Tokenized_Headlines</th>\n",
       "      <th>Normal_Tokenized_Headlines</th>\n",
       "      <th>index_colon_apply</th>\n",
       "      <th>index_say_apply</th>\n",
       "      <th>tokens_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jim cramer: a better way to invest in the covi...</td>\n",
       "      <td>[jim, cramer, :, a, better, way, to, invest, i...</td>\n",
       "      <td>[jim, cramer, :, better, way, invest, vaccine,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cramer's lightning round: i would own teradyne</td>\n",
       "      <td>[cramer, s, lightning, round, :, i, would, own...</td>\n",
       "      <td>[cramer, lightning, round, :, would, teradyne]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cramer's week ahead: big week for earnings, ev...</td>\n",
       "      <td>[cramer, s, week, ahead, :, big, week, for, ea...</td>\n",
       "      <td>[cramer, week, ahead, :, big, week, earnings, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iq capital ceo keith bliss says tech and healt...</td>\n",
       "      <td>[iq, capital, ceo, keith, bliss, says, tech, a...</td>\n",
       "      <td>[iq, capital, ceo, keith, bliss, say, tech, he...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wall street delivered the 'kind of pullback i'...</td>\n",
       "      <td>[wall, street, delivered, the, kind, of, pullb...</td>\n",
       "      <td>[wall, street, delivered, kind, pullback, wait...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0  jim cramer: a better way to invest in the covi...   \n",
       "1     cramer's lightning round: i would own teradyne   \n",
       "2  cramer's week ahead: big week for earnings, ev...   \n",
       "3  iq capital ceo keith bliss says tech and healt...   \n",
       "4  wall street delivered the 'kind of pullback i'...   \n",
       "\n",
       "                                 Tokenized_Headlines  \\\n",
       "0  [jim, cramer, :, a, better, way, to, invest, i...   \n",
       "1  [cramer, s, lightning, round, :, i, would, own...   \n",
       "2  [cramer, s, week, ahead, :, big, week, for, ea...   \n",
       "3  [iq, capital, ceo, keith, bliss, says, tech, a...   \n",
       "4  [wall, street, delivered, the, kind, of, pullb...   \n",
       "\n",
       "                          Normal_Tokenized_Headlines  index_colon_apply  \\\n",
       "0  [jim, cramer, :, better, way, invest, vaccine,...                  2   \n",
       "1     [cramer, lightning, round, :, would, teradyne]                  3   \n",
       "2  [cramer, week, ahead, :, big, week, earnings, ...                  3   \n",
       "3  [iq, capital, ceo, keith, bliss, say, tech, he...                  0   \n",
       "4  [wall, street, delivered, kind, pullback, wait...                  0   \n",
       "\n",
       "   index_say_apply  tokens_len  \n",
       "0                0           8  \n",
       "1                0           5  \n",
       "2                0          10  \n",
       "3                5           8  \n",
       "4                8           8  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "851087b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_colon_say(news, index1, index3):\n",
    "    Headlines_without_colon = []\n",
    "    Headlines_without_say = []\n",
    "    for i in range(len(news)):\n",
    "        if index3[i] - index1[i] <= 1:\n",
    "            Headlines_without_colon.append(news[i][:index1[i]])\n",
    "        else:\n",
    "            Headlines_without_colon.append(news[i][index1[i]+1:])\n",
    "            \n",
    "    return Headlines_without_colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e4aafbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Headlines_without_colon = remove_colon_say(headlines_data['Normal_Tokenized_Headlines'],  headlines_data['index_colon_apply']\n",
    "                                        ,headlines_data['tokens_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "86290d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_data['Headlines_without_colon'] = Headlines_without_colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "75f12b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "withou_colon_len = []\n",
    "for i in headlines_data.Headlines_without_colon:\n",
    "    withou_colon_len.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d7b0cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_data['withou_colon_len'] = withou_colon_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3b5e3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_colon = headlines_data.Headlines_without_colon.tolist()\n",
    "tokens =  headlines_data.tokens_len.tolist()\n",
    "len_diff = []\n",
    "for i in range(len(without_colon)):\n",
    "    len_diff.append(tokens[i]-len(without_colon[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f5244063",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_data['len_diff'] = len_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b6f86fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Tokenized_Headlines</th>\n",
       "      <th>Normal_Tokenized_Headlines</th>\n",
       "      <th>index_colon_apply</th>\n",
       "      <th>index_say_apply</th>\n",
       "      <th>tokens_len</th>\n",
       "      <th>Headlines_without_colon</th>\n",
       "      <th>withou_colon_len</th>\n",
       "      <th>len_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jim cramer: a better way to invest in the covi...</td>\n",
       "      <td>[jim, cramer, :, a, better, way, to, invest, i...</td>\n",
       "      <td>[jim, cramer, :, better, way, invest, vaccine,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[better, way, invest, vaccine, gold, rush]</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cramer's lightning round: i would own teradyne</td>\n",
       "      <td>[cramer, s, lightning, round, :, i, would, own...</td>\n",
       "      <td>[cramer, lightning, round, :, would, teradyne]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[would, teradyne]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cramer's week ahead: big week for earnings, ev...</td>\n",
       "      <td>[cramer, s, week, ahead, :, big, week, for, ea...</td>\n",
       "      <td>[cramer, week, ahead, :, big, week, earnings, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[big, week, earnings, even, bigger, week, vacc...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iq capital ceo keith bliss says tech and healt...</td>\n",
       "      <td>[iq, capital, ceo, keith, bliss, says, tech, a...</td>\n",
       "      <td>[iq, capital, ceo, keith, bliss, say, tech, he...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[capital, ceo, keith, bliss, say, tech, health...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wall street delivered the 'kind of pullback i'...</td>\n",
       "      <td>[wall, street, delivered, the, kind, of, pullb...</td>\n",
       "      <td>[wall, street, delivered, kind, pullback, wait...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[street, delivered, kind, pullback, waiting, j...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53365</th>\n",
       "      <td>malaysia says never hired british data firm at...</td>\n",
       "      <td>[malaysia, says, never, hired, british, data, ...</td>\n",
       "      <td>[malaysia, say, never, hired, british, data, f...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>[say, never, hired, british, data, firm, cente...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53366</th>\n",
       "      <td>prosecutors search volkswagen headquarters in ...</td>\n",
       "      <td>[prosecutors, search, volkswagen, headquarters...</td>\n",
       "      <td>[prosecutor, search, volkswagen, headquarters,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>[search, volkswagen, headquarters, new, emissi...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53367</th>\n",
       "      <td>mcdonald's sets greenhouse gas reduction targets</td>\n",
       "      <td>[mcdonald, s, sets, greenhouse, gas, reduction...</td>\n",
       "      <td>[mcdonald, set, greenhouse, gas, reduction, ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[set, greenhouse, gas, reduction, target]</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53368</th>\n",
       "      <td>pratt &amp; whitney to deliver spare a320neo engin...</td>\n",
       "      <td>[pratt, &amp;, whitney, to, deliver, spare, a320ne...</td>\n",
       "      <td>[pratt, whitney, deliver, spare, engine, soon,...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[pratt, whitney, deliver, spare, engine, soon,...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53369</th>\n",
       "      <td>uk will always consider ways to improve data l...</td>\n",
       "      <td>[uk, will, always, consider, ways, to, improve...</td>\n",
       "      <td>[uk, always, consider, way, improve, data, law...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[pm, may, spokesman]</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53370 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Headlines  \\\n",
       "0      jim cramer: a better way to invest in the covi...   \n",
       "1         cramer's lightning round: i would own teradyne   \n",
       "2      cramer's week ahead: big week for earnings, ev...   \n",
       "3      iq capital ceo keith bliss says tech and healt...   \n",
       "4      wall street delivered the 'kind of pullback i'...   \n",
       "...                                                  ...   \n",
       "53365  malaysia says never hired british data firm at...   \n",
       "53366  prosecutors search volkswagen headquarters in ...   \n",
       "53367   mcdonald's sets greenhouse gas reduction targets   \n",
       "53368  pratt & whitney to deliver spare a320neo engin...   \n",
       "53369  uk will always consider ways to improve data l...   \n",
       "\n",
       "                                     Tokenized_Headlines  \\\n",
       "0      [jim, cramer, :, a, better, way, to, invest, i...   \n",
       "1      [cramer, s, lightning, round, :, i, would, own...   \n",
       "2      [cramer, s, week, ahead, :, big, week, for, ea...   \n",
       "3      [iq, capital, ceo, keith, bliss, says, tech, a...   \n",
       "4      [wall, street, delivered, the, kind, of, pullb...   \n",
       "...                                                  ...   \n",
       "53365  [malaysia, says, never, hired, british, data, ...   \n",
       "53366  [prosecutors, search, volkswagen, headquarters...   \n",
       "53367  [mcdonald, s, sets, greenhouse, gas, reduction...   \n",
       "53368  [pratt, &, whitney, to, deliver, spare, a320ne...   \n",
       "53369  [uk, will, always, consider, ways, to, improve...   \n",
       "\n",
       "                              Normal_Tokenized_Headlines  index_colon_apply  \\\n",
       "0      [jim, cramer, :, better, way, invest, vaccine,...                  2   \n",
       "1         [cramer, lightning, round, :, would, teradyne]                  3   \n",
       "2      [cramer, week, ahead, :, big, week, earnings, ...                  3   \n",
       "3      [iq, capital, ceo, keith, bliss, say, tech, he...                  0   \n",
       "4      [wall, street, delivered, kind, pullback, wait...                  0   \n",
       "...                                                  ...                ...   \n",
       "53365  [malaysia, say, never, hired, british, data, f...                  0   \n",
       "53366  [prosecutor, search, volkswagen, headquarters,...                  0   \n",
       "53367  [mcdonald, set, greenhouse, gas, reduction, ta...                  0   \n",
       "53368  [pratt, whitney, deliver, spare, engine, soon,...                  8   \n",
       "53369  [uk, always, consider, way, improve, data, law...                  7   \n",
       "\n",
       "       index_say_apply  tokens_len  \\\n",
       "0                    0           8   \n",
       "1                    0           5   \n",
       "2                    0          10   \n",
       "3                    5           8   \n",
       "4                    8           8   \n",
       "...                ...         ...   \n",
       "53365                1           8   \n",
       "53366                0           6   \n",
       "53367                0           5   \n",
       "53368                0           9   \n",
       "53369                0          10   \n",
       "\n",
       "                                 Headlines_without_colon  withou_colon_len  \\\n",
       "0             [better, way, invest, vaccine, gold, rush]                 6   \n",
       "1                                      [would, teradyne]                 2   \n",
       "2      [big, week, earnings, even, bigger, week, vacc...                 7   \n",
       "3      [capital, ceo, keith, bliss, say, tech, health...                 8   \n",
       "4      [street, delivered, kind, pullback, waiting, j...                 8   \n",
       "...                                                  ...               ...   \n",
       "53365  [say, never, hired, british, data, firm, cente...                 8   \n",
       "53366  [search, volkswagen, headquarters, new, emissi...                 6   \n",
       "53367          [set, greenhouse, gas, reduction, target]                 5   \n",
       "53368  [pratt, whitney, deliver, spare, engine, soon,...                 8   \n",
       "53369                               [pm, may, spokesman]                 3   \n",
       "\n",
       "       len_diff  \n",
       "0             2  \n",
       "1             3  \n",
       "2             3  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "53365         0  \n",
       "53366         0  \n",
       "53367         0  \n",
       "53368         1  \n",
       "53369         7  \n",
       "\n",
       "[53370 rows x 9 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "21ea02e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-174-bfeff03dceee>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headlines_data.Headlines_without_colon[i] = headlines_data.Normal_Tokenized_Headlines[i]\n"
     ]
    }
   ],
   "source": [
    "len_diff = headlines_data['len_diff'].tolist()\n",
    "for i in range(len(len_diff)):\n",
    "    if len_diff[i] >= 5:\n",
    "         headlines_data.Headlines_without_colon[i] = headlines_data.Normal_Tokenized_Headlines[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "005fde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_tokenized_further(tokens) -> list:\n",
    "    '''further data cleanning'''\n",
    "    Tokens = []\n",
    "    for token in tokens:\n",
    "        Tokens.append([wnl.lemmatize(word).lower()\n",
    "                                           for word in token \n",
    "                                           if word.lower() not in stopwords and\n",
    "                                           word.isalpha() and word not in special_punc\n",
    "                                           and word not in high_freq_useless]) \n",
    "    return Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fb6a2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens = normal_tokenized_further(headlines_data.Headlines_without_colon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5f2f851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_data['Tokens'] = Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3fc39d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = nltk.FreqDist(word for sent in headlines_data.Headlines_without_colon for word in sent)\n",
    "# freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9273480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_headlines_data = headlines_data.loc[:,['Tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0cea3a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[better, way, invest, vaccine, gold, rush]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[would, teradyne]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[big, week, earnings, even, bigger, week, vacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[capital, keith, bliss, tech, healthcare, rally]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[street, delivered, kind, pullback, waiting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53365</th>\n",
       "      <td>[never, hired, british, data, firm, center, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53366</th>\n",
       "      <td>[search, volkswagen, headquarters, new, emissi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53367</th>\n",
       "      <td>[set, greenhouse, gas, reduction, target]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53368</th>\n",
       "      <td>[pratt, whitney, deliver, spare, engine, soon,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53369</th>\n",
       "      <td>[uk, always, consider, way, improve, data, law...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53370 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens\n",
       "0             [better, way, invest, vaccine, gold, rush]\n",
       "1                                      [would, teradyne]\n",
       "2      [big, week, earnings, even, bigger, week, vacc...\n",
       "3       [capital, keith, bliss, tech, healthcare, rally]\n",
       "4           [street, delivered, kind, pullback, waiting]\n",
       "...                                                  ...\n",
       "53365  [never, hired, british, data, firm, center, sc...\n",
       "53366  [search, volkswagen, headquarters, new, emissi...\n",
       "53367          [set, greenhouse, gas, reduction, target]\n",
       "53368  [pratt, whitney, deliver, spare, engine, soon,...\n",
       "53369  [uk, always, consider, way, improve, data, law...\n",
       "\n",
       "[53370 rows x 1 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_headlines_data\n",
    "# use for sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c8cab6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fc3a00fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5d8f15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "postagging = [nltk.pos_tag(sent) for sent in new_headlines_data['Tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aa28b47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.875\n",
      "1        0.000\n",
      "2        0.375\n",
      "3        0.250\n",
      "4        0.000\n",
      "         ...  \n",
      "53365   -0.625\n",
      "53366    0.375\n",
      "53367    0.000\n",
      "53368    0.125\n",
      "53369    0.000\n",
      "Name: senti_score, Length: 53370, dtype: float64\n",
      "<bound method NDFrame.head of                                                   Tokens  \\\n",
      "0             [better, way, invest, vaccine, gold, rush]   \n",
      "1                                      [would, teradyne]   \n",
      "2      [big, week, earnings, even, bigger, week, vacc...   \n",
      "3       [capital, keith, bliss, tech, healthcare, rally]   \n",
      "4           [street, delivered, kind, pullback, waiting]   \n",
      "...                                                  ...   \n",
      "53365  [never, hired, british, data, firm, center, sc...   \n",
      "53366  [search, volkswagen, headquarters, new, emissi...   \n",
      "53367          [set, greenhouse, gas, reduction, target]   \n",
      "53368  [pratt, whitney, deliver, spare, engine, soon,...   \n",
      "53369  [uk, always, consider, way, improve, data, law...   \n",
      "\n",
      "                                                 pos_tag  senti_score  \n",
      "0      [(better, RBR), (way, NN), (invest, JJ), (vacc...        0.875  \n",
      "1                          [(would, MD), (teradyne, VB)]        0.000  \n",
      "2      [(big, JJ), (week, NN), (earnings, NNS), (even...        0.375  \n",
      "3      [(capital, NN), (keith, NN), (bliss, JJ), (tec...        0.250  \n",
      "4      [(street, NN), (delivered, VBD), (kind, NN), (...        0.000  \n",
      "...                                                  ...          ...  \n",
      "53365  [(never, RB), (hired, VBN), (british, JJ), (da...       -0.625  \n",
      "53366  [(search, NN), (volkswagen, NN), (headquarters...        0.375  \n",
      "53367  [(set, VBN), (greenhouse, NN), (gas, NN), (red...        0.000  \n",
      "53368  [(pratt, NN), (whitney, NN), (deliver, NN), (s...        0.125  \n",
      "53369  [(uk, JJ), (always, RB), (consider, VBP), (way...        0.000  \n",
      "\n",
      "[53370 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "pos=neg=obj=count=0\n",
    "new_headlines_data['pos_tag'] = postagging\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def get_sentiment(word,tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    \n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    #Lemmatization\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
    "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
    "    #Some of the words have only one Synset and some have several.\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
    "\n",
    "    pos=neg=obj=count=0\n",
    "    \n",
    "    ###################################################################################\n",
    "senti_score = []\n",
    "\n",
    "for pos_val in new_headlines_data['pos_tag']:\n",
    "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
    "    for score in senti_val:\n",
    "        try:\n",
    "            pos = pos + score[1]  #positive score is stored at 2nd position\n",
    "            neg = neg + score[2]  #negative score is stored at 3rd position\n",
    "        except:\n",
    "            continue\n",
    "    senti_score.append(pos - neg)\n",
    "    pos=neg=0    \n",
    "    \n",
    "new_headlines_data['senti_score'] = senti_score\n",
    "print(new_headlines_data['senti_score'])\n",
    "\n",
    "print(new_headlines_data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c5346051",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall=[]\n",
    "for i in range(len(new_headlines_data)):\n",
    "    if new_headlines_data['senti_score'][i]> 0:\n",
    "        overall.append('Positive')\n",
    "    elif new_headlines_data['senti_score'][i]< 0:\n",
    "        overall.append('Negative')\n",
    "    else:\n",
    "        overall.append('Neutral')\n",
    "new_headlines_data['Overall Sentiment']=overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "26ac87c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>senti_score</th>\n",
       "      <th>Overall Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[better, way, invest, vaccine, gold, rush]</td>\n",
       "      <td>[(better, RBR), (way, NN), (invest, JJ), (vacc...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[would, teradyne]</td>\n",
       "      <td>[(would, MD), (teradyne, VB)]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[big, week, earnings, even, bigger, week, vacc...</td>\n",
       "      <td>[(big, JJ), (week, NN), (earnings, NNS), (even...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[capital, keith, bliss, tech, healthcare, rally]</td>\n",
       "      <td>[(capital, NN), (keith, NN), (bliss, JJ), (tec...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[street, delivered, kind, pullback, waiting]</td>\n",
       "      <td>[(street, NN), (delivered, VBD), (kind, NN), (...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53365</th>\n",
       "      <td>[never, hired, british, data, firm, center, sc...</td>\n",
       "      <td>[(never, RB), (hired, VBN), (british, JJ), (da...</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53366</th>\n",
       "      <td>[search, volkswagen, headquarters, new, emissi...</td>\n",
       "      <td>[(search, NN), (volkswagen, NN), (headquarters...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53367</th>\n",
       "      <td>[set, greenhouse, gas, reduction, target]</td>\n",
       "      <td>[(set, VBN), (greenhouse, NN), (gas, NN), (red...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53368</th>\n",
       "      <td>[pratt, whitney, deliver, spare, engine, soon,...</td>\n",
       "      <td>[(pratt, NN), (whitney, NN), (deliver, NN), (s...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53369</th>\n",
       "      <td>[uk, always, consider, way, improve, data, law...</td>\n",
       "      <td>[(uk, JJ), (always, RB), (consider, VBP), (way...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53370 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tokens  \\\n",
       "0             [better, way, invest, vaccine, gold, rush]   \n",
       "1                                      [would, teradyne]   \n",
       "2      [big, week, earnings, even, bigger, week, vacc...   \n",
       "3       [capital, keith, bliss, tech, healthcare, rally]   \n",
       "4           [street, delivered, kind, pullback, waiting]   \n",
       "...                                                  ...   \n",
       "53365  [never, hired, british, data, firm, center, sc...   \n",
       "53366  [search, volkswagen, headquarters, new, emissi...   \n",
       "53367          [set, greenhouse, gas, reduction, target]   \n",
       "53368  [pratt, whitney, deliver, spare, engine, soon,...   \n",
       "53369  [uk, always, consider, way, improve, data, law...   \n",
       "\n",
       "                                                 pos_tag  senti_score  \\\n",
       "0      [(better, RBR), (way, NN), (invest, JJ), (vacc...        0.875   \n",
       "1                          [(would, MD), (teradyne, VB)]        0.000   \n",
       "2      [(big, JJ), (week, NN), (earnings, NNS), (even...        0.375   \n",
       "3      [(capital, NN), (keith, NN), (bliss, JJ), (tec...        0.250   \n",
       "4      [(street, NN), (delivered, VBD), (kind, NN), (...        0.000   \n",
       "...                                                  ...          ...   \n",
       "53365  [(never, RB), (hired, VBN), (british, JJ), (da...       -0.625   \n",
       "53366  [(search, NN), (volkswagen, NN), (headquarters...        0.375   \n",
       "53367  [(set, VBN), (greenhouse, NN), (gas, NN), (red...        0.000   \n",
       "53368  [(pratt, NN), (whitney, NN), (deliver, NN), (s...        0.125   \n",
       "53369  [(uk, JJ), (always, RB), (consider, VBP), (way...        0.000   \n",
       "\n",
       "      Overall Sentiment  \n",
       "0              Positive  \n",
       "1               Neutral  \n",
       "2              Positive  \n",
       "3              Positive  \n",
       "4               Neutral  \n",
       "...                 ...  \n",
       "53365          Negative  \n",
       "53366          Positive  \n",
       "53367           Neutral  \n",
       "53368          Positive  \n",
       "53369           Neutral  \n",
       "\n",
       "[53370 rows x 4 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_headlines_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0f6e1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.concat([news_data, new_headlines_data], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3fc1a82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>pos_tag</th>\n",
       "      <th>senti_score</th>\n",
       "      <th>Overall Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Jim Cramer: A better way to invest in the Covi...</td>\n",
       "      <td>7:51  PM ET Fri, 17 July 2020</td>\n",
       "      <td>[better, way, invest, vaccine, gold, rush]</td>\n",
       "      <td>[(better, RBR), (way, NN), (invest, JJ), (vacc...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Cramer's lightning round: I would own Teradyne</td>\n",
       "      <td>7:33  PM ET Fri, 17 July 2020</td>\n",
       "      <td>[would, teradyne]</td>\n",
       "      <td>[(would, MD), (teradyne, VB)]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Cramer's week ahead: Big week for earnings, ev...</td>\n",
       "      <td>7:25  PM ET Fri, 17 July 2020</td>\n",
       "      <td>[big, week, earnings, even, bigger, week, vacc...</td>\n",
       "      <td>[(big, JJ), (week, NN), (earnings, NNS), (even...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>IQ Capital CEO Keith Bliss says tech and healt...</td>\n",
       "      <td>4:24  PM ET Fri, 17 July 2020</td>\n",
       "      <td>[capital, keith, bliss, tech, healthcare, rally]</td>\n",
       "      <td>[(capital, NN), (keith, NN), (bliss, JJ), (tec...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Wall Street delivered the 'kind of pullback I'...</td>\n",
       "      <td>7:36  PM ET Thu, 16 July 2020</td>\n",
       "      <td>[street, delivered, kind, pullback, waiting]</td>\n",
       "      <td>[(street, NN), (delivered, VBD), (kind, NN), (...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53365</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Malaysia says never hired British data firm at...</td>\n",
       "      <td>Mar 20 2018</td>\n",
       "      <td>[never, hired, british, data, firm, center, sc...</td>\n",
       "      <td>[(never, RB), (hired, VBN), (british, JJ), (da...</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53366</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Prosecutors search Volkswagen headquarters in ...</td>\n",
       "      <td>Mar 20 2018</td>\n",
       "      <td>[search, volkswagen, headquarters, new, emissi...</td>\n",
       "      <td>[(search, NN), (volkswagen, NN), (headquarters...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53367</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>McDonald's sets greenhouse gas reduction targets</td>\n",
       "      <td>Mar 20 2018</td>\n",
       "      <td>[set, greenhouse, gas, reduction, target]</td>\n",
       "      <td>[(set, VBN), (greenhouse, NN), (gas, NN), (red...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53368</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>Pratt &amp; Whitney to deliver spare A320neo engin...</td>\n",
       "      <td>Mar 20 2018</td>\n",
       "      <td>[pratt, whitney, deliver, spare, engine, soon,...</td>\n",
       "      <td>[(pratt, NN), (whitney, NN), (deliver, NN), (s...</td>\n",
       "      <td>0.125</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53369</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>UK will always consider ways to improve data l...</td>\n",
       "      <td>Mar 20 2018</td>\n",
       "      <td>[uk, always, consider, way, improve, data, law...</td>\n",
       "      <td>[(uk, JJ), (always, RB), (consider, VBP), (way...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53370 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year                                          Headlines  \\\n",
       "0     2020-01-01  Jim Cramer: A better way to invest in the Covi...   \n",
       "1     2020-01-01     Cramer's lightning round: I would own Teradyne   \n",
       "2     2020-01-01  Cramer's week ahead: Big week for earnings, ev...   \n",
       "3     2020-01-01  IQ Capital CEO Keith Bliss says tech and healt...   \n",
       "4     2020-01-01  Wall Street delivered the 'kind of pullback I'...   \n",
       "...          ...                                                ...   \n",
       "53365 2018-01-01  Malaysia says never hired British data firm at...   \n",
       "53366 2018-01-01  Prosecutors search Volkswagen headquarters in ...   \n",
       "53367 2018-01-01   McDonald's sets greenhouse gas reduction targets   \n",
       "53368 2018-01-01  Pratt & Whitney to deliver spare A320neo engin...   \n",
       "53369 2018-01-01  UK will always consider ways to improve data l...   \n",
       "\n",
       "                                 Time  \\\n",
       "0       7:51  PM ET Fri, 17 July 2020   \n",
       "1       7:33  PM ET Fri, 17 July 2020   \n",
       "2       7:25  PM ET Fri, 17 July 2020   \n",
       "3       4:24  PM ET Fri, 17 July 2020   \n",
       "4       7:36  PM ET Thu, 16 July 2020   \n",
       "...                               ...   \n",
       "53365                     Mar 20 2018   \n",
       "53366                     Mar 20 2018   \n",
       "53367                     Mar 20 2018   \n",
       "53368                     Mar 20 2018   \n",
       "53369                     Mar 20 2018   \n",
       "\n",
       "                                                  Tokens  \\\n",
       "0             [better, way, invest, vaccine, gold, rush]   \n",
       "1                                      [would, teradyne]   \n",
       "2      [big, week, earnings, even, bigger, week, vacc...   \n",
       "3       [capital, keith, bliss, tech, healthcare, rally]   \n",
       "4           [street, delivered, kind, pullback, waiting]   \n",
       "...                                                  ...   \n",
       "53365  [never, hired, british, data, firm, center, sc...   \n",
       "53366  [search, volkswagen, headquarters, new, emissi...   \n",
       "53367          [set, greenhouse, gas, reduction, target]   \n",
       "53368  [pratt, whitney, deliver, spare, engine, soon,...   \n",
       "53369  [uk, always, consider, way, improve, data, law...   \n",
       "\n",
       "                                                 pos_tag  senti_score  \\\n",
       "0      [(better, RBR), (way, NN), (invest, JJ), (vacc...        0.875   \n",
       "1                          [(would, MD), (teradyne, VB)]        0.000   \n",
       "2      [(big, JJ), (week, NN), (earnings, NNS), (even...        0.375   \n",
       "3      [(capital, NN), (keith, NN), (bliss, JJ), (tec...        0.250   \n",
       "4      [(street, NN), (delivered, VBD), (kind, NN), (...        0.000   \n",
       "...                                                  ...          ...   \n",
       "53365  [(never, RB), (hired, VBN), (british, JJ), (da...       -0.625   \n",
       "53366  [(search, NN), (volkswagen, NN), (headquarters...        0.375   \n",
       "53367  [(set, VBN), (greenhouse, NN), (gas, NN), (red...        0.000   \n",
       "53368  [(pratt, NN), (whitney, NN), (deliver, NN), (s...        0.125   \n",
       "53369  [(uk, JJ), (always, RB), (consider, VBP), (way...        0.000   \n",
       "\n",
       "      Overall Sentiment  \n",
       "0              Positive  \n",
       "1               Neutral  \n",
       "2              Positive  \n",
       "3              Positive  \n",
       "4               Neutral  \n",
       "...                 ...  \n",
       "53365          Negative  \n",
       "53366          Positive  \n",
       "53367           Neutral  \n",
       "53368          Positive  \n",
       "53369           Neutral  \n",
       "\n",
       "[53370 rows x 7 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a9a6e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data.to_excel('labeled_data_11_29.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea9dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216.156px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
